// import { GoogleGenerativeAI, GenerativeModel, GenerationConfig, SafetySetting, HarmCategory, HarmBlockThreshold } from "@google/generative-ai";
// import { ENV } from "../config/env";

// // Initialize the Google Generative AI with your API key or use a fallback for demo mode
// const genAI = new GoogleGenerativeAI(ENV.GEMINI_API_KEY);
// console.log(ENV.GEMINI_API_KEY);

// // Default configuration for generation
// const defaultConfig: GenerationConfig = {
//   temperature: 0.7,
//   topK: 40,
//   topP: 0.95,
//   maxOutputTokens: 1024,
// };

// // Default safety settings
// const safetySettings: SafetySetting[] = [
//   {
//     category: HarmCategory.HARM_CATEGORY_HARASSMENT,
//     threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE
//   },
//   {
//     category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
//     threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE
//   },
//   {
//     category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
//     threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE
//   },
//   {
//     category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
//     threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE
//   }
// ];

// export type ChatMessage = {
//   role: 'user' | 'model';
//   parts: string;
// };

// export type StreamCallbacks = {
//   onStart?: () => void;
//   onToken?: (token: string) => void;
//   onComplete?: (fullResponse: string) => void;
//   onError?: (error: Error) => void;
// };

// // Estimated token counts for different content types
// const TOKEN_LIMIT = 30000; // Conservative estimate for Gemini models
// const RESERVE_TOKENS = 1000; // Reserve tokens for the response

// // Persistent chat sessions
// type ChatSession = {
//   chat: any; // GenerativeModel.startChat result
//   personaId: string;
//   lastActive: number;
// };

// class GeminiService {
//   private model: GenerativeModel;
//   private isDemoMode: boolean;
//   private streamAbortController: AbortController | null = null;
//   private chatSessions: Map<string, ChatSession> = new Map();
  
//   constructor(modelName: string = "gemini-2.0-pro-exp-02-05") {
//     this.isDemoMode = !ENV.GEMINI_API_KEY || ENV.GEMINI_API_KEY === 'demo_mode';
    
//     try {
//       this.model = genAI.getGenerativeModel({ model: modelName });
//       console.log("Gemini model initialized successfully");
//     } catch (error) {
//       console.error("Error initializing Gemini model:", error);
//       // Create a placeholder model for demo mode
//       this.model = {} as GenerativeModel;
//     }
    
//     // Clean up expired sessions every 5 minutes
//     setInterval(() => this.cleanupExpiredSessions(), 5 * 60 * 1000);
//   }

//   // Clean up sessions that haven't been used in the last hour
//   private cleanupExpiredSessions() {
//     const now = Date.now();
//     const expirationTime = 60 * 60 * 1000; // 1 hour
    
//     for (const [chatId, session] of this.chatSessions.entries()) {
//       if (now - session.lastActive > expirationTime) {
//         this.chatSessions.delete(chatId);
//         console.log(`Cleaning up expired chat session: ${chatId}`);
//       }
//     }
//   }
  
//   // Get or create a chat session for a specific chat ID
//   private async getOrCreateChatSession(chatId: string, personaId: string, systemPrompt: string): Promise<any> {
//     // If we have an existing session for this chat ID and the same persona
//     if (this.chatSessions.has(chatId)) {
//       const session = this.chatSessions.get(chatId)!;
      
//       // If the persona changed, create a new session
//       if (session.personaId !== personaId) {
//         console.log(`Persona changed from ${session.personaId} to ${personaId}, creating new session`);
//         return this.createNewChatSession(chatId, personaId, systemPrompt);
//       }
      
//       // Update the last active timestamp
//       session.lastActive = Date.now();
//       console.log(`Using existing chat session for ${chatId}`);
//       return session.chat;
//     }
    
//     // Create a new session if one doesn't exist
//     return this.createNewChatSession(chatId, personaId, systemPrompt);
//   }
  
//   // Create a new chat session
//   private async createNewChatSession(chatId: string, personaId: string, systemPrompt: string): Promise<any> {
//     console.log(`Creating new chat session for ${chatId} with persona ${personaId}`);
    
//     // Create the chat session
//     const chat = this.model.startChat({
//       generationConfig: defaultConfig,
//       safetySettings,
//       history: [],
//     });
    
//     // Send the system prompt
//     await chat.sendMessage(systemPrompt);
//     console.log("System prompt sent to new chat session");
    
//     // Store the session
//     this.chatSessions.set(chatId, {
//       chat,
//       personaId,
//       lastActive: Date.now(),
//     });
    
//     return chat;
//   }

//   // Estimate token count for a message (very rough estimation)
//   private estimateTokenCount(text: string): number {
//     // A very rough estimate: 1 token â‰ˆ 4 characters for English
//     // This is just a heuristic - actual tokenization depends on the model
//     return Math.ceil(text.length / 4);
//   }
  
//   // Prune message history to fit token limit
//   private pruneMessageHistory(messages: ChatMessage[], systemPrompt: string): ChatMessage[] {
//     // Always keep system prompt + most recent messages
//     const systemPromptTokens = this.estimateTokenCount(systemPrompt);
//     const availableTokens = TOKEN_LIMIT - systemPromptTokens - RESERVE_TOKENS;
    
//     let totalTokens = 0;
//     const prunedMessages: ChatMessage[] = [];
    
//     // Start from the most recent messages and work backwards
//     for (let i = messages.length - 1; i >= 0; i--) {
//       const message = messages[i];
//       const messageTokens = this.estimateTokenCount(message.parts);
      
//       // If adding this message would exceed our limit, stop
//       if (totalTokens + messageTokens > availableTokens) {
//         // Insert a summary message at the start if we had to prune
//         if (i > 0) {
//           console.log(`Pruned ${i + 1} older messages due to token limits`);
//           prunedMessages.unshift({
//             role: 'model',
//             parts: "...(Earlier parts of the conversation were summarized to save space)..."
//           });
//         }
//         break;
//       }
      
//       // Add this message to our pruned list and continue
//       totalTokens += messageTokens;
//       prunedMessages.unshift(message); // Add to the start to maintain order
//     }
    
//     console.log(`Final message count after pruning: ${prunedMessages.length}, estimated tokens: ${totalTokens}`);
//     return prunedMessages;
//   }

//   // Cancel any ongoing stream request
//   cancelStream() {
//     if (this.streamAbortController) {
//       console.log("Cancelling ongoing stream");
//       this.streamAbortController.abort();
//       this.streamAbortController = null;
//     }
//   }

//   // Generate content (non-streaming)
//   async generateContent(prompt: string, config?: Partial<GenerationConfig>) {
//     if (this.isDemoMode) {
//       return this.generateDemoResponse(prompt);
//     }
    
//     try {
//       const generationConfig = {
//         ...defaultConfig,
//         ...config,
//       };
      
//       const result = await this.model.generateContent({
//         contents: [{ role: 'user', parts: [{ text: prompt }] }],
//         generationConfig,
//         safetySettings,
//       });
//       return result.response.text();
//     } catch (error) {
//       console.error("Error generating content:", error);
//       throw error;
//     }
//   }
  
//   // Chat with history (non-streaming)
//   async chatWithHistory(messages: ChatMessage[], systemPrompt: string, config?: Partial<GenerationConfig>) {
//     if (this.isDemoMode) {
//       return this.generateDemoResponse(messages[messages.length - 1]?.parts || "");
//     }
    
//     try {
//       // Create a chat session
//       const chat = this.model.startChat({
//         generationConfig: {
//           ...defaultConfig,
//           ...config,
//         },
//         safetySettings,
//         history: [],  // Start with empty history as we'll send messages manually
//       });

//       // Send the system prompt first
//       await chat.sendMessage(systemPrompt);
      
//       // Now send all messages in the history
//       let result;
//       for (const message of messages) {
//         if (message.role === 'user') {
//           result = await chat.sendMessage(message.parts);
//         }
//       }
      
//       return result?.response.text() || "No response generated.";
//     } catch (error) {
//       console.error("Error in chat with history:", error);
//       throw error;
//     }
//   }
  
//   // Stream chat with history maintaining a continuous session
//   async streamChatWithHistory(
//     chatId: string,
//     messages: ChatMessage[],
//     systemPrompt: string,
//     personaId: string,
//     callbacks: StreamCallbacks,
//     config?: Partial<GenerationConfig>
//   ) {
//     console.log(`Starting streamChatWithHistory for chat ${chatId}, persona ${personaId}`);
    
//     // Cancel any ongoing stream before starting a new one
//     this.cancelStream();
    
//     if (this.isDemoMode) {
//       console.log("Using demo mode for streaming");
//       return this.streamDemoResponse(messages[messages.length - 1]?.parts || "", callbacks);
//     }
    
//     try {
//       // Create new abort controller for this stream
//       this.streamAbortController = new AbortController();
//       const signal = this.streamAbortController.signal;
      
//       // Start callback
//       if (callbacks.onStart) {
//         callbacks.onStart();
//       }
      
//       // Get or create a chat session for this chat ID
//       const chat = await this.getOrCreateChatSession(chatId, personaId, systemPrompt);
      
//       // Prune the message history to fit token limits
//       const prunedMessages = this.pruneMessageHistory(messages, systemPrompt);
      
//       // Find the last user message
//       const lastUserMessage = [...prunedMessages].reverse().find(msg => msg.role === 'user');
//       if (!lastUserMessage) {
//         throw new Error("No user message found in history");
//       }
      
//       // Stream the response
//       let fullResponse = '';
      
//       try {
//         console.log("Starting message stream");
//         const result = await chat.sendMessageStream(lastUserMessage.parts);
        
//         for await (const chunk of result.stream) {
//           // Check if streaming was canceled
//           if (signal.aborted) {
//             console.log("Stream was aborted, breaking");
//             break;
//           }
          
//           const chunkText = chunk.text();
//           fullResponse += chunkText;
          
//           // Call token callback if provided
//           if (callbacks.onToken && !signal.aborted) {
//             callbacks.onToken(chunkText);
//           }
//         }
        
//         // Only call onComplete if not aborted
//         if (!signal.aborted && callbacks.onComplete) {
//           callbacks.onComplete(fullResponse);
//         }
        
//         // Update the session's last active timestamp
//         if (this.chatSessions.has(chatId)) {
//           this.chatSessions.get(chatId)!.lastActive = Date.now();
//         }
        
//         // Clear the abort controller
//         this.streamAbortController = null;
        
//         return fullResponse;
        
//       } catch (error) {
//         console.error("Error during streaming:", error);
        
//         // If we hit a context length error, try recreating the session with only the latest message
//         if (error instanceof Error && 
//             (error.message.includes("context length") || 
//              error.message.includes("token limit"))) {
//           console.log("Token limit exceeded, recreating session with reduced history");
          
//           // Remove the failed chat session
//           this.chatSessions.delete(chatId);
          
//           // Create a new session with just the system prompt
//           const newChat = await this.createNewChatSession(chatId, personaId, systemPrompt);
          
//           // Send only the last user message
//           console.log("Retrying with only the last user message");
//           const retryResult = await newChat.sendMessageStream(lastUserMessage.parts);
          
//           // Handle the retry stream
//           let retryResponse = '';
//           for await (const chunk of retryResult.stream) {
//             if (signal.aborted) break;
            
//             const chunkText = chunk.text();
//             retryResponse += chunkText;
            
//             if (callbacks.onToken && !signal.aborted) {
//               callbacks.onToken(chunkText);
//             }
//           }
          
//           if (!signal.aborted && callbacks.onComplete) {
//             callbacks.onComplete(retryResponse);
//           }
          
//           return retryResponse;
//         } else {
//           // For other errors, rethrow
//           throw error;
//         }
//       }
//     } catch (error) {
//       // Don't report error if it was due to an intentional cancellation
//       if (error instanceof Error && error.name === 'AbortError') {
//         console.log("Stream was canceled");
//         return "";
//       }
      
//       console.error("Error in streaming chat:", error);
//       if (callbacks.onError) callbacks.onError(error as Error);
      
//       // Clean up the session on error to avoid lingering issues
//       this.chatSessions.delete(chatId);
      
//       throw error;
//     }
//   }
  
//   // Demo mode method for non-streaming responses
//   private async generateDemoResponse(prompt: string): Promise<string> {
//     return new Promise((resolve) => {
//       setTimeout(() => {
//         resolve(`This is a demo response to: "${prompt}". For real AI responses, please configure your Gemini API key.`);
//       }, 1000);
//     });
//   }
  
//   // Demo mode method for streaming responses
//   private async streamDemoResponse(prompt: string, callbacks: StreamCallbacks): Promise<string> {
//     return new Promise((resolve) => {
//       // Create a new abort controller for this demo stream
//       this.streamAbortController = new AbortController();
//       const signal = this.streamAbortController.signal;
      
//       // Call onStart callback if provided
//       if (callbacks.onStart) {
//         console.log("Demo: Calling onStart callback");
//         callbacks.onStart();
//       }
      
//       // Prepare the demo response
//       const demoResponse = `This is a demo response to: "${prompt}". For real AI responses, please configure your Gemini API key.`;
//       let fullResponse = '';
//       let index = 0;
      
//       console.log("Demo: Starting character stream simulation");
      
//       // Function to stream the next character
//       const streamNextChar = () => {
//         // Stop if aborted
//         if (signal.aborted) {
//           console.log("Demo: Stream was aborted");
//           resolve(fullResponse);
//           return;
//         }
        
//         // If we still have characters to send
//         if (index < demoResponse.length) {
//           // Get the next character
//           const chunk = demoResponse.charAt(index);
//           fullResponse += chunk;
          
//           // Call the token callback if provided
//           if (callbacks.onToken) {
//             callbacks.onToken(chunk);
//           }
          
//           // Move to next character
//           index++;
          
//           // Schedule the next character
//           setTimeout(streamNextChar, 30);
//         } else {
//           // We're done, call complete and resolve
//           console.log("Demo: Stream complete");
//           if (callbacks.onComplete) {
//             callbacks.onComplete(fullResponse);
//           }
//           this.streamAbortController = null;
//           resolve(fullResponse);
//         }
//       };
      
//       // Start the streaming process
//       streamNextChar();
//     });
//   }
// }

// export default new GeminiService();

// // Re-export everything from the modular gemini service implementation
// export * from './gemini';
// export { default } from './gemini';
